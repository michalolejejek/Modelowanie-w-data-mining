{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7OTY-MbVn_j"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "#cfar10 seafar10\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL 1\n",
        "# Architektura trzywarstwowa:\n",
        "# - Warstwa wejściowa - 13 cech\n",
        "# - Warstwa ukryta: 64 neurony, ReLU\n",
        "# - Warstwa wyjściowa: 3 klasy, softmax\n",
        "# Uczenie:\n",
        "# - Gradient descent, cross-entropy loss, 1000 epok, learning rate 0.01\n",
        "# - Wagi aktualizowane backpropagation\n",
        "# Dane:\n",
        "# - Standaryzowane (StandardScaler), etykiety kodowane jako one-hot\n",
        "# Ewaluacja poprzez accuracy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target #Etykiety\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Podział danych\n",
        "\n",
        "#Standaryzacja inputowanych cech\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "y_train = pd.get_dummies(y_train).values #One-hot encoding dla etykiet\n",
        "y_test = pd.get_dummies(y_test).values\n",
        "\n",
        "#Kodowanie parametrów\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 64\n",
        "output_size = y_train.shape[1]\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "#Losowe wartości dla biasów i inicjalizacja wag\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "#Funkcje aktywacji i ich pochodne\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "#Funkcja softmax do normalizacji wyników wyjściowych\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "#Funkcja kosztu - entropia krzyżowa\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    loss = -np.sum(y_true * np.log(y_pred)) / m\n",
        "    return loss\n",
        "\n",
        "#Pochodna funkcji kosztu\n",
        "def cross_entropy_loss_derivative(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    return (y_pred - y_true) / m\n",
        "\n",
        "#Propagacja w przód\n",
        "def forward_prop(X):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "#Propagacja wsteczna - obliczenie gradientów\n",
        "def backward_prop(X, y, Z1, A1, Z2, A2):\n",
        "    m = y.shape[0]\n",
        "    dZ2 = cross_entropy_loss_derivative(y, A2)\n",
        "    dW2 = np.dot(A1.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "#Trening modelu\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    X_train, y_train = shuffle(X_train, y_train)\n",
        "    Z1, A1, Z2, A2 = forward_prop(X_train)\n",
        "    loss = cross_entropy_loss(y_train, A2)\n",
        "    dW1, db1, dW2, db2 = backward_prop(X_train, y_train, Z1, A1, Z2, A2)\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss}') #Wyswietlamy straty co 100 epok\n",
        "\n",
        "#Ocena Modelu\n",
        "Z1, A1, Z2, A2 = forward_prop(X_test)\n",
        "predictions = np.argmax(A2, axis=1)\n",
        "accuracy = np.mean(predictions == np.argmax(y_test, axis=1))\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f\"Shape of dW1: {dW1.shape}\")\n",
        "print(f\"Shape of dW2: {dW2.shape}\")\n",
        "print(f\"Shape of db1: {db1.shape}\")\n",
        "print(f\"Shape of db2: {db2.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjhG2X1MXC5_",
        "outputId": "aba54493-f16a-40b4-ba31-45126b95b1f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.0990333421960812\n",
            "Epoch 100, Loss: 1.0858826418522525\n",
            "Epoch 200, Loss: 1.062233137278481\n",
            "Epoch 300, Loss: 0.9913663178403466\n",
            "Epoch 400, Loss: 0.8042442226049584\n",
            "Epoch 500, Loss: 0.5363205693288589\n",
            "Epoch 600, Loss: 0.33637801015270813\n",
            "Epoch 700, Loss: 0.22207649424792386\n",
            "Epoch 800, Loss: 0.1598383279270626\n",
            "Epoch 900, Loss: 0.12361077441774491\n",
            "Accuracy: 1.0\n",
            "Shape of dW1: (13, 64)\n",
            "Shape of dW2: (64, 3)\n",
            "Shape of db1: (1, 64)\n",
            "Shape of db2: (1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Porównajmy to z gotowym modelem z biblioteki."
      ],
      "metadata": {
        "id": "zcDHLX-TQ63c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Dropout, Flatten\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9s-sy4xRDiy",
        "outputId": "3ac762a8-4f98-4f92-befc-7a6156ffbf7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - accuracy: 0.3948 - loss: 1.1423 - val_accuracy: 0.2500 - val_loss: 1.1130\n",
            "Epoch 2/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2795 - loss: 1.1448 - val_accuracy: 0.3889 - val_loss: 1.0544\n",
            "Epoch 3/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4081 - loss: 1.1020 - val_accuracy: 0.4444 - val_loss: 1.0215\n",
            "Epoch 4/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4077 - loss: 1.0884 - val_accuracy: 0.4722 - val_loss: 0.9956\n",
            "Epoch 5/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.4063 - loss: 1.0747 - val_accuracy: 0.5278 - val_loss: 0.9773\n",
            "Epoch 6/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4998 - loss: 1.0220 - val_accuracy: 0.5556 - val_loss: 0.9644\n",
            "Epoch 7/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4825 - loss: 1.0176 - val_accuracy: 0.6111 - val_loss: 0.9469\n",
            "Epoch 8/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5814 - loss: 0.9540 - val_accuracy: 0.6111 - val_loss: 0.9264\n",
            "Epoch 9/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5627 - loss: 0.9686 - val_accuracy: 0.6111 - val_loss: 0.9060\n",
            "Epoch 10/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5392 - loss: 0.9408 - val_accuracy: 0.6667 - val_loss: 0.8952\n",
            "Epoch 11/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6137 - loss: 0.9459 - val_accuracy: 0.7222 - val_loss: 0.8670\n",
            "Epoch 12/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6374 - loss: 0.9046 - val_accuracy: 0.7500 - val_loss: 0.8496\n",
            "Epoch 13/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5628 - loss: 0.9660 - val_accuracy: 0.7500 - val_loss: 0.8380\n",
            "Epoch 14/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.5474 - loss: 0.9473 - val_accuracy: 0.7222 - val_loss: 0.8265\n",
            "Epoch 15/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6268 - loss: 0.9287 - val_accuracy: 0.7222 - val_loss: 0.8187\n",
            "Epoch 16/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6426 - loss: 0.9087 - val_accuracy: 0.7500 - val_loss: 0.8052\n",
            "Epoch 17/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6719 - loss: 0.8342 - val_accuracy: 0.7500 - val_loss: 0.7838\n",
            "Epoch 18/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6320 - loss: 0.8562 - val_accuracy: 0.7778 - val_loss: 0.7736\n",
            "Epoch 19/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6319 - loss: 0.8076 - val_accuracy: 0.7778 - val_loss: 0.7602\n",
            "Epoch 20/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5987 - loss: 0.8601 - val_accuracy: 0.7222 - val_loss: 0.7622\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7923eaec2b50>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL 2\n",
        "# Architektura trzywarstwowa z pamięcią rekurencyjną:\n",
        "# - Warstwa wejściowa: 13 cech\n",
        "# - Warstwa ukryta: 64 neurony, sigmoid, rekurencyjne połączenie (RNN)\n",
        "# - Warstwa wyjściowa: 3 klasy, softmax\n",
        "# Uczenie:\n",
        "# - Gradient descent, cross-entropy loss, 1000 epok, learning rate 0.01\n",
        "# - Wagi aktualizowane backpropagation przez czas (BPTT)\n",
        "# Dane:\n",
        "# - Standaryzowane (StandardScaler), etykiety kodowane jako one-hot\n",
        "# Ewaluacja poprzez accuracy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target #Etykiety\n",
        "\n",
        "#Podział danych\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Standaryzacja danych\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#Zamiana etykiet na format one-hot\n",
        "y_train = pd.get_dummies(y_train).values\n",
        "y_test = pd.get_dummies(y_test).values\n",
        "\n",
        "#Parametry sieci\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 64\n",
        "output_size = y_train.shape[1]\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "#Losowe wartości dla wag i inicjalizacja biasów\n",
        "np.random.seed(42)\n",
        "Wxh = np.random.randn(input_size, hidden_size) * 0.01\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Why = np.random.randn(hidden_size, output_size) * 0.01\n",
        "bh = np.zeros((1, hidden_size))\n",
        "by = np.zeros((1, output_size))\n",
        "\n",
        "#Funkcje aktywacji i ich pochodne\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    loss = -np.sum(y_true * np.log(y_pred)) / m\n",
        "    return loss\n",
        "\n",
        "def cross_entropy_loss_derivative(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    return (y_pred - y_true) / m\n",
        "\n",
        "#Propagacja w przód\n",
        "def forward_prop(X):\n",
        "    T = X.shape[0]\n",
        "    h = np.zeros((T, hidden_size))\n",
        "    y = np.zeros((T, output_size))\n",
        "    for t in range(T):\n",
        "        h[t] = sigmoid(np.dot(X[t], Wxh) + np.dot(h[t-1], Whh) + bh)\n",
        "        y[t] = softmax(np.dot(h[t], Why) + by)\n",
        "    return h, y\n",
        "\n",
        "#Propagacja wstecz\n",
        "def backward_prop(X, y_true, h, y_pred):\n",
        "    T = X.shape[0]\n",
        "    dWxh = np.zeros_like(Wxh)\n",
        "    dWhh = np.zeros_like(Whh)\n",
        "    dWhy = np.zeros_like(Why)\n",
        "    dbh = np.zeros_like(bh)  # dB1 (1, 64)\n",
        "    dby = np.zeros_like(by)  # dB2 (1, 3)\n",
        "    dh_next = np.zeros_like(h[0])\n",
        "\n",
        "    for t in reversed(range(T)):\n",
        "        dy = cross_entropy_loss_derivative(y_true[t], y_pred[t])\n",
        "        dWhy += np.dot(h[t].reshape(-1, 1), dy.reshape(1, -1))\n",
        "        dby += dy\n",
        "        dh = np.dot(dy, Why.T) + dh_next\n",
        "        dh_raw = dh * sigmoid_derivative(h[t])\n",
        "        dWxh += np.outer(X[t], dh_raw)\n",
        "        dWhh += np.outer(h[t-1], dh_raw)\n",
        "        dbh += np.sum(dh_raw, axis=0, keepdims=True)\n",
        "        dh_next = np.dot(dh_raw, Whh.T)\n",
        "\n",
        "    return dWxh, dWhh, dWhy, dbh, dby\n",
        "\n",
        "\n",
        "\n",
        "#Trening sieci\n",
        "for epoch in range(epochs):\n",
        "    X_train, y_train = shuffle(X_train, y_train)\n",
        "    h, y_pred = forward_prop(X_train)\n",
        "    loss = cross_entropy_loss(y_train, y_pred)\n",
        "    dWxh, dWhh, dWhy, dbh, dby = backward_prop(X_train, y_train, h, y_pred)\n",
        "\n",
        "    Wxh -= learning_rate * dWxh\n",
        "    Whh -= learning_rate * dWhh\n",
        "    Why -= learning_rate * dWhy\n",
        "    bh -= learning_rate * dbh\n",
        "    by -= learning_rate * dby\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss}') #Wyswietlamy straty co 100 epok\n",
        "\n",
        "#Ocena Modelu 2\n",
        "h_test, y_pred_test = forward_prop(X_test)\n",
        "predictions = np.argmax(y_pred_test, axis=1)\n",
        "accuracy = np.mean(predictions == np.argmax(y_test, axis=1))\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f\"Shape of dWxh: {dWxh.shape}\")\n",
        "print(f\"Shape of dWhh: {dWhh.shape}\")\n",
        "print(f\"Shape of dWhy: {dWhy.shape}\")\n",
        "print(f\"Shape of dbh: {dbh.shape}\")\n",
        "print(f\"Shape of dby: {dby.shape}\")\n"
      ],
      "metadata": {
        "id": "LqVtNhnvYMu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83f87398-5c74-4e37-9f78-4fe0efb6db52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.0978546456696723\n",
            "Epoch 100, Loss: 0.07254076990086356\n",
            "Epoch 200, Loss: 0.03038129298114878\n",
            "Epoch 300, Loss: 0.018968754743870435\n",
            "Epoch 400, Loss: 0.013830673502336714\n",
            "Epoch 500, Loss: 0.010754300534812399\n",
            "Epoch 600, Loss: 0.008345419254853166\n",
            "Epoch 700, Loss: 0.00977247841329793\n",
            "Epoch 800, Loss: 0.006083849475685241\n",
            "Epoch 900, Loss: 0.0052779105390970684\n",
            "Accuracy: 1.0\n",
            "Shape of dWxh: (13, 64)\n",
            "Shape of dWhh: (64, 64)\n",
            "Shape of dWhy: (64, 3)\n",
            "Shape of dbh: (1, 64)\n",
            "Shape of dby: (1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL 3\n",
        "# Architektura trzywarstwowa z rekurencyjnym RNN i Dropout:\n",
        "# - Warstwa wejściowa: 13 cech, tensor o wymiarach (1, 13)\n",
        "# - Warstwa ukryta 1: 64 neurony, ReLU, rekurencyjne połączenie (SimpleRNN)\n",
        "# - Warstwa ukryta 2: 64 neurony, ReLU\n",
        "# - Dropout: 50%\n",
        "# - Warstwa wyjściowa: 3 klasy, softmax\n",
        "# Uczenie:\n",
        "# - Adam, sparse categorical crossentropy\n",
        "# - 20 epok, batch size 16\n",
        "# Dane:\n",
        "# - Standaryzowane (StandardScaler)\n",
        "# Ewaluacja poprzez accuracy\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Dropout\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=1)\n",
        "X_test = np.expand_dims(X_test, axis=1)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=64, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhQErYDkR_Hr",
        "outputId": "25eb1fc5-1201-4d16-a91e-cea9a7c1bf24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.4422 - loss: 1.0707 - val_accuracy: 0.7500 - val_loss: 0.8628\n",
            "Epoch 2/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6676 - loss: 0.8358 - val_accuracy: 0.9722 - val_loss: 0.6645\n",
            "Epoch 3/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8359 - loss: 0.6136 - val_accuracy: 0.9722 - val_loss: 0.5070\n",
            "Epoch 4/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8649 - loss: 0.5058 - val_accuracy: 0.9722 - val_loss: 0.3772\n",
            "Epoch 5/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9105 - loss: 0.4031 - val_accuracy: 1.0000 - val_loss: 0.2777\n",
            "Epoch 6/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9105 - loss: 0.3121 - val_accuracy: 1.0000 - val_loss: 0.2036\n",
            "Epoch 7/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9768 - loss: 0.2151 - val_accuracy: 1.0000 - val_loss: 0.1538\n",
            "Epoch 8/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9738 - loss: 0.1800 - val_accuracy: 1.0000 - val_loss: 0.1179\n",
            "Epoch 9/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9931 - loss: 0.1455 - val_accuracy: 1.0000 - val_loss: 0.0903\n",
            "Epoch 10/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9839 - loss: 0.1109 - val_accuracy: 1.0000 - val_loss: 0.0727\n",
            "Epoch 11/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9909 - loss: 0.1218 - val_accuracy: 1.0000 - val_loss: 0.0604\n",
            "Epoch 12/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9549 - loss: 0.1034 - val_accuracy: 1.0000 - val_loss: 0.0504\n",
            "Epoch 13/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9705 - loss: 0.1075 - val_accuracy: 1.0000 - val_loss: 0.0411\n",
            "Epoch 14/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9969 - loss: 0.0591 - val_accuracy: 1.0000 - val_loss: 0.0348\n",
            "Epoch 15/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9511 - loss: 0.1331 - val_accuracy: 1.0000 - val_loss: 0.0292\n",
            "Epoch 16/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9879 - loss: 0.0830 - val_accuracy: 1.0000 - val_loss: 0.0262\n",
            "Epoch 17/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9946 - loss: 0.0576 - val_accuracy: 1.0000 - val_loss: 0.0244\n",
            "Epoch 18/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9969 - loss: 0.0444 - val_accuracy: 1.0000 - val_loss: 0.0217\n",
            "Epoch 19/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9978 - loss: 0.0597 - val_accuracy: 1.0000 - val_loss: 0.0176\n",
            "Epoch 20/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9879 - loss: 0.0518 - val_accuracy: 1.0000 - val_loss: 0.0148\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7923e63605d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL 4\n",
        "# Architektura sieci w pełni połączonej (MLP) z trzema warstwami ukrytymi:\n",
        "# - Warstwa wejściowa: 13 cech\n",
        "# - Warstwa ukryta 1: 128 neuronów, sigmoid\n",
        "# - Warstwa ukryta 2: 64 neurony, sigmoid\n",
        "# - Warstwa ukryta 3: 32 neurony, sigmoid\n",
        "# - Warstwa wyjściowa: 3 klasy, sigmoid\n",
        "# Uczenie:\n",
        "# - MSE jako funkcja kosztu\n",
        "# - Gradient descent, learning rate 0.01\n",
        "# - 1000 epok\n",
        "# Dane:\n",
        "# - Standaryzowane (StandardScaler), etykiety kodowane jako one-hot\n",
        "# Ewaluacja poprzez accuracy\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target.reshape(-1, 1)\n",
        "\n",
        "encoder = LabelBinarizer()\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 128\n",
        "hidden_size2 = 64\n",
        "hidden_size3 = 32\n",
        "output_size = y_train.shape[1]\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(input_size, hidden_size1)\n",
        "b1 = np.zeros((1, hidden_size1))\n",
        "W2 = np.random.randn(hidden_size1, hidden_size2)\n",
        "b2 = np.zeros((1, hidden_size2))\n",
        "W3 = np.random.randn(hidden_size2, hidden_size3)\n",
        "b3 = np.zeros((1, hidden_size3))\n",
        "W4 = np.random.randn(hidden_size3, output_size)\n",
        "b4 = np.zeros((1, output_size))\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    Z1 = np.dot(X_train, W1) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    Z3 = np.dot(A2, W3) + b3\n",
        "    A3 = sigmoid(Z3)\n",
        "    Z4 = np.dot(A3, W4) + b4\n",
        "    y_pred = sigmoid(Z4)\n",
        "\n",
        "    loss = mse_loss(y_train, y_pred)\n",
        "\n",
        "    dZ4 = y_pred - y_train\n",
        "    dW4 = np.dot(A3.T, dZ4)\n",
        "    db4 = np.sum(dZ4, axis=0, keepdims=True)\n",
        "\n",
        "    dA3 = np.dot(dZ4, W4.T)\n",
        "    dZ3 = dA3 * sigmoid_derivative(A3)\n",
        "    dW3 = np.dot(A2.T, dZ3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    dA2 = np.dot(dZ3, W3.T)\n",
        "    dZ2 = dA2 * sigmoid_derivative(A2)\n",
        "    dW2 = np.dot(A1.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * sigmoid_derivative(A1)\n",
        "    dW1 = np.dot(X_train.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    W4 -= learning_rate * dW4\n",
        "    b4 -= learning_rate * db4\n",
        "    W3 -= learning_rate * dW3\n",
        "    b3 -= learning_rate * db3\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "Z1_test = np.dot(X_test, W1) + b1\n",
        "A1_test = sigmoid(Z1_test)\n",
        "Z2_test = np.dot(A1_test, W2) + b2\n",
        "A2_test = sigmoid(Z2_test)\n",
        "Z3_test = np.dot(A2_test, W3) + b3\n",
        "A3_test = sigmoid(Z3_test)\n",
        "Z4_test = np.dot(A3_test, W4) + b4\n",
        "y_pred_test = sigmoid(Z4_test)\n",
        "\n",
        "accuracy = np.mean(np.argmax(y_pred_test, axis=1) == np.argmax(y_test, axis=1))\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c11eRghOj-M",
        "outputId": "10447e5a-5869-4162-a184-2a5907708f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.34420283056183315\n",
            "Epoch 100, Loss: 0.00013130261646672188\n",
            "Epoch 200, Loss: 1.8673934510593826e-05\n",
            "Epoch 300, Loss: 6.581888411918572e-06\n",
            "Epoch 400, Loss: 3.12576916735045e-06\n",
            "Epoch 500, Loss: 1.7702072893331888e-06\n",
            "Epoch 600, Loss: 1.1237032933968243e-06\n",
            "Epoch 700, Loss: 7.716132659666734e-07\n",
            "Epoch 800, Loss: 5.61167450586117e-07\n",
            "Epoch 900, Loss: 4.263300052691548e-07\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-wEFqA8QQ9h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}